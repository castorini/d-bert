{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Task1: Sentiment Analysis","metadata":{}},{"cell_type":"markdown","source":"## Importing the dataset","metadata":{}},{"cell_type":"code","source":"# SST-2\n\nfrom datasets import load_dataset\n# df_train = pd.DataFrame(load_dataset('glue', 'sst2', split='train')).fillna('')\n# df_test = pd.DataFrame(load_dataset('glue', 'sst2', split='test')).fillna('')\n# df_valid = pd.DataFrame(load_dataset('glue', 'sst2', split='validation')).fillna('')\n\ndf_train = pd.DataFrame(load_dataset('glue', 'cola', split='train')).fillna('')\ndf_test = pd.DataFrame(load_dataset('glue', 'cola', split='test')).fillna('')\ndf_valid = pd.DataFrame(load_dataset('glue', 'cola', split='validation')).fillna('')\ny_train = df_train['label']\ny_valid = df_valid['label']\ny_test = df_test['label']\ndf_train = df_train['sentence']\ndf_valid = df_valid['sentence']\ndf_test = df_test['sentence']\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extended dataset loading\ndfe = pd.read_csv('../input/kdextended/extended-dataset/sst2_extended.tsv', sep='\\t')\ndf_train = dfe['sentence']\ny_train = dfe['label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extended dataset splitting\n# from sklearn.model_selection import train_test_split\n\n# df_train, df_test, y_train, y_test = train_test_split(df_train, y_train,\n#                                    random_state=104, \n#                                    test_size=0.2, \n#                                    shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building deep learning model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nmax_words=10000\ntokenizer=Tokenizer(max_words)\ntokenizer.fit_on_texts(df_train)\nsequence_train=tokenizer.texts_to_sequences(df_train)\nsequence_valid=tokenizer.texts_to_sequences(df_valid)\nsequence_test=tokenizer.texts_to_sequences(df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word2vec=tokenizer.word_index\nV=len(word2vec)\nprint('dataset has %s number of independent tokens' %V)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\ndata_train=pad_sequences(sequence_train)\ndata_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"T=data_train.shape[1]\ndata_valid=pad_sequences(sequence_valid,maxlen=T)\ndata_test=pad_sequences(sequence_test,maxlen=T)\ndata_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input,Conv1D,MaxPooling1D,Dense,GlobalMaxPooling1D,Embedding,Bidirectional,LSTM,Dropout\nfrom tensorflow.keras.models import Model, Sequential, model_from_json\nfrom tensorflow.keras import utils","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"D=20\ni=Input((T,))\nx=Embedding(V+1,D)(i)\nx=Conv1D(32,3,activation='relu')(x)\nx=MaxPooling1D(3)(x)\nx=Conv1D(64,3,activation='relu')(x)\nx=MaxPooling1D(3)(x)\nx=Conv1D(128,3,activation='relu')(x)\nx=GlobalMaxPooling1D()(x)\nx=Dense(1,activation='sigmoid')(x)\nmodel=Model(i,x)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the model (CNN)","metadata":{}},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\ncnn_senti=model.fit(data_train,y_train,validation_data=(data_test,y_test),epochs=10,batch_size=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the model (BiLSTM)","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nD=20\nmodel.add(Embedding(V+1, D, input_length=T))\nlstm_out = 64\nmodel.add(Bidirectional(LSTM(lstm_out)))\nmodel.add(Dropout(.1, input_shape=(64,)))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dropout(.1, input_shape=(10,)))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='RMSProp', metrics=['accuracy'])\nmodel.summary()\n# utils.plot_model(model, show_shapes=False, expand_nested=True, to_file='sentiment.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training = model.fit(data_train,y_train,epochs=10,batch_size=128)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"from matplotlib import rc\nimport matplotlib.pyplot as plt\n\nrc('font',**{'family':'serif','serif':['Palatino']})\n\ndef plot_training(training, filename, ymin=0.5, valid=True):\n    plt.figure(figsize=(16, 5), dpi=300)\n    plt.subplot(1,2,1)\n    plt.plot(training.history['accuracy'], label='accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    if valid:\n        plt.plot(training.history['val_accuracy'], color='red', linestyle='dashed')\n        plt.legend(['train', 'validation'])\n    plt.ylim([ymin, 1])\n\n    plt.subplot(1,2,2)\n    plt.plot(training.history['loss'])\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    if valid:\n        plt.plot(training.history['val_loss'], color='red', linestyle='dashed')\n        plt.legend(['train', 'validation'])\n    plt.savefig(filename, bbox='tight')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_training(training, 'cola.pdf', ymin=0.7, valid=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(data_train, y_train, batch_size = 1024, use_multiprocessing=True, workers = -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(data_test, y_test, batch_size = 1024, use_multiprocessing=True, workers = -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(data_valid, y_valid, batch_size = 1024, use_multiprocessing=True, workers = -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(data_test, batch_size=1024, verbose= 1)\ny_pred = np.round(y_pred)\npred = pd.DataFrame(y_pred)\npred['index'] = pred.index\nc = pred.columns\npred[[c[0], c[1]]] = pred[[c[1], c[0]]]\npred.columns = ['index', 'prediction']\npred.to_csv(\"COLA.tsv\", sep='\\t', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Task2: Semantic Textual Similarity","metadata":{}},{"cell_type":"code","source":"!pip install datasets","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nsts_train = pd.DataFrame(load_dataset('glue', 'stsb', split='train')).fillna('')\nsts_eval = pd.DataFrame(load_dataset('glue', 'stsb', split='validation')).fillna('')\nsts_test = pd.DataFrame(load_dataset('glue', 'stsb', split='test')).fillna('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extended dataset loading\ndfe = pd.read_csv('../input/kdextended/extended-dataset/stsb_extended.tsv', sep='\\t')\nsts_train = dfe[['sentence1', 'sentence2']]\ny_train = dfe['label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\n\ndef remove_stopwords(dataset):\n    data = dataset.copy()\n    data[['sentence1', 'sentence2']] = dataset[['sentence1', 'sentence2']].apply(lambda x: x.astype(str).str.lower())  \n    data['sentence1'] = data.apply(lambda row: nltk.word_tokenize(row['sentence1']), axis=1).apply(lambda x: [item for item in x if item not in stopwords.words('english')])\n    data['sentence2'] = data.apply(lambda row: nltk.word_tokenize(row['sentence2']), axis=1).apply(lambda x: [item for item in x if item not in stopwords.words('english')])\n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sts_train_stop = remove_stopwords(sts_train)\nsts_test_stop = remove_stopwords(sts_test)\nsts_eval_stop = remove_stopwords(sts_eval)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_document_frequency(df):\n    document_frequency_dict = {}\n    all_sentences =  df[[\"sentence1\", \"sentence2\"]]\n    sentences = all_sentences.values.flatten().tolist()\n    n = len(sentences)\n\n    for s in sentences:\n        for token in set(s):\n            document_frequency_dict[token] = document_frequency_dict.get(token, 0) + 1\n\n    return document_frequency_dict, n\n\ndocument_frequencies, num_documents = get_document_frequency(sts_train_stop)\nnum_documents","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim import models\nword2vec_path = '../input/gnewsvector/GoogleNews-vectors-negative300.bin'\nword2vec_model = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\nimport math\ndef average_sentence_embedding(tokens, embedding_model):\n    tokens = [i for i in tokens if i in embedding_model]\n\n    if len(tokens) == 0:\n        return []\n\n    count = Counter(tokens)\n    token_list = list(count)\n    term_frequency = [count[i] / len(tokens) for i in token_list]\n\n    inv_doc_frequency = [\n        math.log(num_documents / (document_frequencies.get(i, 0) + 1)) for i in count\n    ]\n\n    word_embeddings = [embedding_model[token] for token in token_list]\n    weights = [term_frequency[i] * inv_doc_frequency[i] for i in range(len(token_list))]\n    return list(np.average(word_embeddings, weights=weights, axis=0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.spatial import distance\ndef calculate_cosine_similarity(embedding1, embedding2):\n    cosine_similarity = 1 - distance.cosine(embedding1, embedding2)\n    return cosine_similarity","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def average_word_embedding_cosine_similarity(df, embedding_model):\n    df['sentence1_embedding'] = df.apply(lambda x: average_sentence_embedding(x.sentence1, embedding_model), axis=1)\n    df['sentence2_embedding'] = df.apply(lambda x: average_sentence_embedding(x.sentence2, embedding_model), axis=1)\n\n    df['predictions'] = df.apply(lambda x: calculate_cosine_similarity(x.sentence1_embedding, x.sentence2_embedding) if \n                                 (sum(x.sentence1_embedding) != 0 and sum(x.sentence2_embedding) != 0) else 0, axis=1)\n    \n    return df['predictions'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = pd.DataFrame(average_word_embedding_cosine_similarity(\n    sts_test_stop, word2vec_model\n))\ny_pred = np.round(MinMaxScaler(feature_range=(0, 5)).fit_transform(y_pred).flatten(), 3)\npred = pd.DataFrame(y_pred)\npred['index'] = pred.index\nc = pred.columns\npred[[c[0], c[1]]] = pred[[c[1], c[0]]]\npred.columns = ['index', 'prediction']\npred.to_csv(\"STS-B.tsv\", sep='\\t', index=False)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nsimilarities = pd.DataFrame(average_word_embedding_cosine_similarity(\n    sts_eval_stop, word2vec_model\n))\n\nsimilarities = MinMaxScaler(feature_range=(0, 5)).fit_transform(similarities).flatten()\nsimilarities\nnp.corrcoef(similarities, sts_eval_stop['label'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import spearmanr\nrho, p = spearmanr(similarities, sts_eval_stop['label'])\nrho","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sts_eval_stop","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Task3: Sentence-pair Classification","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport string\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Embedding, Dense, Flatten, Dropout, TimeDistributed, SimpleRNN, GlobalMaxPooling1D\nfrom keras.metrics import AUC, Accuracy\nfrom keras.models import Sequential\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nimport nltk\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras import utils","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install datasets","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing the dataset","metadata":{}},{"cell_type":"code","source":"# train_data = pd.read_csv(\"../input/multinli-nyu/multinli_1.0/multinli_1.0_train.txt\", sep='\\t', error_bad_lines=False, engine='python')\n# valid_data = pd.read_csv(\"../input/multinli-nyu/multinli_1.0/multinli_1.0_dev_matched.txt\", sep='\\t', error_bad_lines=False, engine='python')\n# # test_data = pd.read_csv(\"../input/stanford-natural-language-inference-corpus/snli_1.0_test.csv\")\n# # validation_data = pd.read_csv(\"../input/stanford-natural-language-inference-corpus/snli_1.0_dev.csv\")\n\n# train_data = train_data[:40000]\n# test_data = test_data[:10000]\n# validation_data = validation_data[:10000]\n\n# dataset = pd.concat([train_data, test_data, validation_data])\n\nfrom datasets import load_dataset\n# train_dataset = load_dataset('glue', 'mrpc', split='train')\n# eval_dataset = load_dataset('glue', 'mrpc', split='validation')\n# test_dataset = load_dataset('glue', 'mrpc', split='test')\n\n# train_dataset = load_dataset('glue', 'rte', split='train')\n# eval_dataset = load_dataset('glue', 'rte', split='validation')\n# test_dataset = load_dataset('glue', 'rte', split='test')\n\ntrain_dataset = load_dataset('glue', 'wnli', split='train')\neval_dataset = load_dataset('glue', 'wnli', split='validation')\ntest_dataset = load_dataset('glue', 'wnli', split='test')\n\n\ntrain = pd.DataFrame(train_dataset)\neval = pd.DataFrame(eval_dataset)\ntest = pd.DataFrame(test_dataset)\nstop_words = stopwords.words('english')","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extended dataset loading\ntrain = pd.read_csv('../input/kdextended/extended-dataset/wnli_extended.tsv', sep='\\t')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.groupby('label', group_keys=False).apply(lambda x: x.sample(943)).sample(frac=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['label'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"dataset = train[['label', 'sentence1', 'sentence2']]\ndataset.dropna(axis=0, inplace=True)\n\nvalid_data = eval[['label', 'sentence1', 'sentence2']]\nvalid_data.dropna(axis=0, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.loc[dataset['label'] != \"-\"]\n# dataset = dataset.loc[dataset['gold_label'] != \"neutral\"]\ndataset = dataset.sample(frac = 1)\n\nvalid_data = valid_data.loc[valid_data['label'] != \"-\"]\n# valid_data = valid_data.loc[valid_data['gold_label'] != \"neutral\"]\nvalid_label = valid_data['label']\n\nsentence1 = dataset['sentence1']\nsentence2 = dataset['sentence2']\n\nlabel = dataset['label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def CleanFeatures(sentences):\n  sentences = sentences.apply(lambda sequence:\n                                            [ltrs.lower() for ltrs in sequence if ltrs not in string.punctuation])\n  sentences = sentences.apply(lambda wrd: ''.join(wrd))\n  sentences = sentences.apply(lambda sequence:\n                                            [word for word in sequence.split() if word not in stop_words])\n  sentences = sentences.apply(lambda wrd: ' '.join(wrd))\n  return sentences\n\nsentence1 = CleanFeatures(sentence1)\nsentence2 = CleanFeatures(sentence2)\nvalid_sentence1 = CleanFeatures(valid_data['sentence1'])\nvalid_sentence2 = CleanFeatures(valid_data['sentence2'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_sentences = np.asarray([sentence1, sentence2])\nall_sentences = all_sentences.reshape(-1,1 )\nall_sentences = all_sentences.reshape(all_sentences.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_sentences.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=6000)\ntokenizer.fit_on_texts(all_sentences)\nsentence1_seq = tokenizer.texts_to_sequences(sentence1)\nsentence1 = pad_sequences(sentence1_seq, maxlen = 100)\n\nsentence2_seq = tokenizer.texts_to_sequences(sentence2)\nsentence2 = pad_sequences(sentence2_seq, maxlen = 100)\n\nvalid_sentence1_seq = tokenizer.texts_to_sequences(valid_sentence1)\nvalid_sentence1 = pad_sequences(valid_sentence1_seq, maxlen = 100)\n\nvalid_sentence2_seq = tokenizer.texts_to_sequences(valid_sentence2)\nvalid_sentence2 = pad_sequences(valid_sentence2_seq, maxlen = 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_ = LabelEncoder()\nlabels = label_.fit_transform(label)\nvocabulary = len(tokenizer.word_index)\nlabels = to_categorical(labels)\n\nvalid_labels = label_.transform(valid_label)\nvalid_labels = to_categorical(valid_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_sentences = np.asarray([sentence1, sentence2])\nall_sentences.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_sentences = np.asarray([valid_sentence1, valid_sentence2])\nvalid_sentences.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_sentences = all_sentences.reshape(all_sentences.shape[1], 2, all_sentences.shape[2])\nall_sentences.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_sentences = valid_sentences.reshape(valid_sentences.shape[1], 2, valid_sentences.shape[2])\nvalid_sentences.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def glove_word_embedding(file_name, vocabulary):\n  embeddings_index = {}\n  file_ = open(file_name)\n  for line in file_:\n      arr = line.split()\n      single_word = arr[0]\n      w = np.asarray(arr[1:],dtype='float32')\n      embeddings_index[single_word] = w\n  file_.close()\n  max_words = vocabulary + 1\n  word_index = tokenizer.word_index\n  embedding_matrix = np.zeros((max_words,50)).astype(object)\n  for word , i in word_index.items():\n          embedding_vector = embeddings_index.get(word)\n          if embedding_vector is not None:\n              embedding_matrix[i] = embedding_vector \n  return embedding_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocabulary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = Sequential()\n# D=100\n# model.add(Embedding(vocabulary+1, D, input_length=train_text.shape[1]))\n# lstm_out = 10\n# model.add(Bidirectional(LSTM(lstm_out)))\n# model.add(Dropout(.1, input_shape=(10,)))\n# model.add(Dense(5, activation='relu'))\n# model.add(Dropout(.1, input_shape=(5,)))\n# model.add(Dense(2, activation='softmax'))\n# model.compile(loss='binary_crossentropy', optimizer='RMSProp', metrics=['accuracy'])\n# model.summary()\n# model.compile(loss=\"binary_crossentropy\",optimizer='adam',metrics=['accuracy'])\n# training = model.fit(train_text,labels,\n#                          epochs = 10,\n#                          use_multiprocessing=True,\n#                          workers=-1,\n# #                          validation_split=0.1,\n# #                          callbacks=[callback],\n# #                          validation_data=(valid_text,valid_labels)\n#                     )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building the deep learning model","metadata":{}},{"cell_type":"code","source":"# model_rnn = tf.keras.models.Sequential()\n# model_rnn.add(Embedding(vocabulary + 1, 50, input_shape=(all_sentences.shape[1], all_sentences.shape[2],)))\n# model_rnn.add(tf.keras.layers.TimeDistributed(SimpleRNN(128, return_sequences=True)))\n# model_rnn.add(tf.keras.layers.Dropout(0.10))\n# model_rnn.add(tf.keras.layers.TimeDistributed(SimpleRNN(128, return_sequences=True)))\n# model_rnn.add(tf.keras.layers.Dropout(0.10))\n# model_rnn.add((tf.keras.layers.TimeDistributed(GlobalMaxPooling1D())))\n# model_rnn.add(tf.keras.layers.Flatten())\n# model_rnn.add(tf.keras.layers.Dense(2, activation='softmax'))\n# model_rnn.layers[0].set_weights([glove_word_embedding(\"../input/glove6b50dtxt/glove.6B.50d.txt\", vocabulary)])\n# model_rnn.layers[0].trainable = False\n# from keras.optimizers import RMSprop\n# model_rnn.compile(loss=\"binary_crossentropy\",optimizer='Adam',metrics=['accuracy'])\n# model_rnn.summary()\n# utils.plot_model(model_rnn, show_shapes=False, expand_nested=True, to_file='sentence-pair.png', show_layer_names=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = Input(shape=(all_sentences.shape[1], all_sentences.shape[2],))\np = TimeDistributed(Embedding(vocabulary + 1, 50))(x)\np = TimeDistributed(LSTM(64, return_sequences=True))(p)\np = TimeDistributed(LSTM(32, return_sequences=True))(p)\nx1 = TimeDistributed(tf.keras.layers.GlobalMaxPooling1D())(p)\nx2 = TimeDistributed(tf.keras.layers.GlobalAveragePooling1D())(p)\nconcat = tf.keras.layers.concatenate([x1, x2])\np = tf.keras.layers.Dropout(0.35)(concat)\np = Flatten()(p)\np = Dense(2, activation=\"softmax\")(p)\n\nm = Model(inputs = x, outputs = p)\nm.layers[1].set_weights([glove_word_embedding(\"../input/glove6b50dtxt/glove.6B.50d.txt\", vocabulary)])\nm.layers[1].trainable = False\nm.compile(loss=\"binary_crossentropy\",optimizer='adam',metrics=[\"accuracy\",])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the model (RNN)","metadata":{}},{"cell_type":"code","source":"class MyThresholdCallback(tf.keras.callbacks.Callback):\n    def __init__(self, threshold):\n        super(MyThresholdCallback, self).__init__()\n        self.threshold = threshold\n\n    def on_epoch_end(self, epoch, logs=None): \n        accuracy = logs[\"accuracy\"]\n        if accuracy >= self.threshold:\n            self.model.stop_training = True\n            \ncallback=MyThresholdCallback(threshold=0.90)\ntraining = m.fit(all_sentences,labels,\n                         epochs = 20,\n                         use_multiprocessing=True,\n                         workers=-1,\n                         callbacks=[callback],\n#                          validation_data=(valid_sentences,valid_labels)\n                    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m.evaluate(valid_sentences, valid_labels, batch_size=1024, workers=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sentence1 = CleanFeatures(test['sentence1'])\ntest_sentence2 = CleanFeatures(test['sentence1'])\n\ntest_sentence1_seq = tokenizer.texts_to_sequences(test_sentence1)\ntest_sentence1 = pad_sequences(test_sentence1_seq, maxlen = 100)\n\ntest_sentence2_seq = tokenizer.texts_to_sequences(test_sentence2)\ntest_sentence2 = pad_sequences(test_sentence2_seq, maxlen = 100)\n\ntest_sentences = np.asarray([test_sentence1, test_sentence2])\ntest_sentences = test_sentences.reshape(test_sentences.shape[1], 2, test_sentences.shape[2])\ntest_sentences.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = m.predict(test_sentences, batch_size=1024, verbose= 1)\ny_pred = np.argmax(y_pred, axis = 1)\npred = pd.DataFrame(y_pred)\npred['index'] = pred.index\nc = pred.columns\npred[[c[0], c[1]]] = pred[[c[1], c[0]]]\npred.columns = ['index', 'prediction']\npred.to_csv(\"WNLI_nokd.tsv\", sep='\\t', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for rte\ny_pred = m.predict(test_sentences, batch_size=512, verbose= 1)\ny_pred = np.argmax(y_pred, axis = 1)\ndef labelize(num):\n    if num == 0:\n        return 'not_entailment'\n    else:\n        return 'entailment'\npred = pd.DataFrame(y_pred)\npred['index'] = pred.index\nc = pred.columns\npred[[c[0], c[1]]] = pred[[c[1], c[0]]]\npred.columns = ['index', 'prediction']\npred['prediction'] = pred['prediction'].apply(lambda x: labelize(x))\npred.to_csv(\"RTE_kd.tsv\", sep='\\t', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"plot_training(training, 'mrpc.pdf')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}